{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066183b9-a357-402e-816b-edaa69af6bbb",
   "metadata": {},
   "source": [
    "Plan : Préparation des données\n",
    "1. Introduction\n",
    "2. Rassemblement des données\n",
    "3. Evaluation des données\n",
    "4. Netoyage des données\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071455bf-a792-4eac-930e-2f1019c3c194",
   "metadata": {},
   "source": [
    "# Préparation des données\n",
    "\n",
    "## 1. INTRODUCTION\n",
    "\n",
    "La préparation des données est une étape très importante en analyse de données. En effet, En tant que data analystes, nous pouvons être confronté à analyser des données de source divers afin de mieux répondre à une problématique. Pour réussir efficacement notre travail, il est primordial de préparer nos données. Cette étape prend en entrer des données très sales et donne en sortie des données propres et de bonnes qualités prés à usage. Étant donné l’importance de cette étape, il a été divisé en trois grandes partis notamment : Collecte, Évaluation et Nettoyage des données.  Dans ces partis il est question pour nous de préparer nos données tout en respecter scrupuleusement les différentes étapes citées plus haut. Autrement, il sera questions de collecter les données sur plusieurs sources différentes, évaluer ces données afin de détecter les problèmes que pose les données et enfin nettoyer les différents problèmes recueillent ces données. Plus loin nous ferons une analyse et visualisation de ces données qui verront l’objet d’un autre document.\n",
    "\n",
    "## 2. Rassemblement des données\n",
    "\n",
    "Rassemblement des données\n",
    "Aussi appelée collecte de données, elle est la première étape dans le processus de préparation des données. Avant cette étape, nous étions démunis de données et après celle-ci, nous en sommes munies. En effet, il s’agit de collecter les toutes les données nécessaires pour la réussite de notre analyse. Il existe deux grandes manières de collecter les données à savoir, la collecte manuelle et la collecte programmatique. La première consiste à se rendre sur la page des données et de les télécharger en des clicks. Par contre la collecte programmatique consiste à collecter les données en utilisant des outils de programmation à l’image de python. Cette seconde méthode est la plus conseillé pour deux raisons : la mise en échelle et la reproductibilité.\n",
    "En ce qui concerne notre projet, il s’agit d’évaluer les données de @WeRateDogs, un utilisateur de tweet. Pour ce faire, nous avons en notre disposition les archives twitter de WeRateDogs dans un fichier nommé twitter_archive_enhanced.csv dont nous devons télécharger manuellement pour la suite de notre projet de collecte. Un autre fichier dont nous avons également accès est image_predictions.tsv, un fichier contenant les prédictions d’image de chat qui est hébergé sur le serveur de Udacity cette adresse :  https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv. En outre nous devons interrogée l’API de twitter avec l’outil tweepy de python pour collecter des données supplémentaires et nécessaires pour notre évaluations. Dans cette partie, le but recherché est de collecter les données de WeRateDogs sur twitter. Pour ce faire, nous devons tout d'abord créer un compte développeur à la suite de la création d’un compte twitter. Une fois ce compte créer, nous devons créer un projet dans lequel nous créons notre première application. C'est cette application qui va nous permettre d'accéder aux données en utilisant L'API de twitter (Tweepy). Cependant à default de ne pas pouvoir collecter des données via l'API de tweeter, Udacity met à notre disposition ces données collectées. Et la suite de notre travail consiste à construire nos dataframe avec ces données de Udacity.\n",
    "\n",
    "## 3. Evaluation des données\n",
    "\n",
    "L'évaluation des données est l'étape situé entre la collecte(rassemblement) et le nettoyage des données. En effet, c'est le précurseur du nettoyage. La réussite du nettoyage dépendant de cette étape. L'évaluation consiste à évaluer les données afin de détecter les données sales et les données désordonnées. Elle peut se faire de deux manières différentes. Nous avons l'évaluation visuelle qui consiste à défiler l'ensemble de dataset afin de repérer les éventuels problèmes. L’évaluation programmatique qui consiste à utiliser des outils comme pandas de python en vue de détecter les problèmes cités plus haut. La seconde méthode bien que recommander, il est possible de combiner les deux car nécessaire et indispensable. Durant l'évaluation de nos données nous avons detecté plus plusieurs problème de qualité et de structure qui seront l'objet des lignes suivantes. \n",
    "\n",
    "### QUALITÉ\n",
    "\n",
    "#### df_clean\n",
    "\n",
    "1. Plus de 181 lignes sont des retweets.\n",
    "2. Il y'a plus de 90% de données manquantes dans les colonnes : retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp.\n",
    "3. user_friends ne possède qu'une seule données '104' représentant les amis de l'utilisateur.\n",
    "4. type de données erronées(timestamp).\n",
    "5. id, created_at, full_text identiques respectivement à tweet_id, timestamp, text.\n",
    "6. user_followers , retweet, favorite sont de type float au lieu de int.\n",
    "7. in_reply_to_status_id, in_reply_to_user_id sont de type float au lieu de int.\n",
    "8. Données manquantes replacées par `None` dans les colonnes (doggo, pupper, puppo, flooter).\n",
    "9. Données manquantes représentées par `None` dans la colonne name.\n",
    "10. Environ 158 lignes ne sont pas des évaluations de chiens.\n",
    "11. Certains lignes évalue plus d'un chien.\n",
    "12. index 705 n'est pas une annotation de chien, mais d’une autre espèce.\n",
    "\n",
    "#### df_images\n",
    "\n",
    "1. Enregistrement manquantes (2075 au lieu de 2356)\n",
    "\n",
    "### ORDRE\n",
    "\n",
    "#### df_clean\n",
    "\n",
    "1. la colonne source présente de données avec des balises HTML contenant à la fois l'url et du texte.\n",
    "2. Dans la colonne expanded_urls, il y'a des lignes qui ont plus des urls.\n",
    "3. les colonnes doggo,floofer, pupper, puppo sont des observations et non des variables.\n",
    "\n",
    "Cette étape nous ait permis de mettre en évidence les différents problèmes de qualité et de structure des données. La suite consistera à nettoyer ces données.\n",
    "\n",
    "## 4. Netoyage des données\n",
    "\n",
    "Le nettoyage des données est la dernière étape après l’évaluation des données. En effet, dans cette étape, le data analystes doit nettoyer tous les problèmes qualités et de structures collectées lors de la phase précédente. Le nettoyage des données se fait en trois étapes principales : Définition, Codage et Test. La Définition consiste à identifier chaque problème puis à le documenter afin que d’autre personnes puissent comprendre notre code. Le Codage consiste à transformer nos définitions en code python exécutable. Et enfin, le Test qui consiste à Tester si le nettoyage c’est bien passé. Nous avons veuillez à respecter ces étapes lors du nettoyage de nos données.\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "La préparation est une phase très importante dans l’analyse des données et bien d’autre. Elle nous permet d’avoir une bonne qualité des données. Elle occupe la majeure partie du temps du processus d’analyse de données. Elle est constituée de trois (3) étapes notamment, la collecte, l’évaluation et le nettoyage. Cependant, il faut noter que la préparation des données est itérative. Après le nettoyage, il convient de réévaluer l’ensemble des données afin de déterminer d’autre problème qui pourraient passer inaperçues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
