{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071455bf-a792-4eac-930e-2f1019c3c194",
   "metadata": {},
   "source": [
    "# Préparation des données\n",
    "## 1. Introduction\n",
    "La préparation des données est une étape très importante en analyse de données et comprend trois grands partis notamment : Collecte, Évaluation et Nettoyage des données. Pour le traitement des données, nous avons collecté les données sur plusieurs sources différentes, évalué ces données afin de détecter les problèmes que posent les données et enfin nettoyé les différents problèmes recessés. \n",
    "## Collecte des données\n",
    "Nous devons travailler sur les données de @WeRateDogs, un utilisateur tweet. Pour ce faire, nous avons en notre disposition les archives twitter de WeRateDogs dans un fichier nommé « twitter_archive_enhanced.csv » dont nous devons télécharger manuellement pour la suite de notre projet de collecte. Un autre fichier dont nous avons également accès est image_predictions.tsv, un fichier contenant les prédictions d’image de chat qui est hébergé sur le serveur de Udacity cette adresse :  https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv. En outre nous devons interrogée l’API de twitter avec l’outil tweepy de python pour collecter des données supplémentaires et nécessaires pour notre évaluations. Dans cette partie, le but recherché est de collecter les données de WeRateDogs sur twitter. Cependant à default de ne pas pouvoir collecter des données via l'API de tweeter, Udacity met à notre disposition ces données collectées.\n",
    "\n",
    "## 3. Évaluation des données\n",
    "L'évaluation des données est l'étape situé entre la collecte(rassemblement) et le nettoyage des données. En effet, c'est le précurseur du nettoyage. L'évaluation consiste à évaluer les données afin de détecter les données sales et les données désordonnées. Durant l'évaluation de nos données nous avons détecté plus plusieurs problèmes de qualité et de structure qui seront l'objet des lignes suivantes. \n",
    "### QUALITÉ\n",
    "#### df_twitter_clean\n",
    "1. Plus de 181 lignes sont des retweets.\n",
    "2. Il y'a plus de 90% de données manquantes dans les colonnes : retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp.\n",
    "3. type de données erronées(timestamp).\n",
    "4. in_reply_to_status_id, in_reply_to_user_id sont de type float au lieu de int.\n",
    "5. Données manquantes replacées par `None` dans les colonnes (doggo, pupper, puppo, flooter).\n",
    "6. Données manquantes représentées par `None` dans la colonne name.\n",
    "7. Environ 158 lignes ne sont pas des évaluations de chiens.\n",
    "8. Certains lignes évalue plus d'un chien.\n",
    "9. index 705 n'est pas une annotation de chien, mais d’une autre espèce.\n",
    "#### df_images_clean\n",
    "10. Enregistrement manquantes (2075 au lieu de 2356)\n",
    "### ORDRE\n",
    "\n",
    "#### df_twitter_clean\n",
    "1. La colonne source présente de données avec des balises HTML contenant à la fois l'url et du texte.\n",
    "2. Dans la colonne expanded_urls, il y'a des lignes qui ont plus des urls.\n",
    "3. Les colonnes doggo,floofer, pupper, puppo sont des observations et non des variables.\n",
    "Cette étape nous ait permis de mettre en évidence les différents problèmes de qualité et de structure des données. La suite consistera à nettoyer ces données.\n",
    "\n",
    "## 4. Nettoyage des données\n",
    "Le nettoyage des données est la dernière étape après l’évaluation des données. Le nettoyage des données se fait en trois étapes principales : Définition, Codage et Test. Nous avons veuillez à respecter ces étapes lors du nettoyage de nos données. Dans cette partie, nous avons eu à nettoyer tous les problèmes de qualités et de structure poser lors de l’évaluation. \n",
    "## 5. Conclusion\n",
    "La préparation est une phase très importante dans l’analyse des données et bien d’autre. Elle nous permet d’avoir une bonne qualité des données. Il s’agissait pour nous de collecter les données de l’utilisateur @WeRateDogs, l’évaluer et nettoyer. Les données collectées présentent de nombreuses valeurs manquantes ce qui constitue un problème pour le nettoyage. À l’issue dut nettoyage, nous avons fusionné les différentes dataset afin de l’enregistrer sous le nom « twitter_archive_master.csv ».\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
